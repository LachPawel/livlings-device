
# ‚úÖ Client Tools Integration Complete!

## What Was Added

### 1. ClientToolsHandler Class ‚úÖ
**File:** `ClientToolsHandler.swift`

Handles 4 client-side tools that the ElevenLabs agent can call:
- `remember(key, value)` - Store conversation memory
- `notify_parent(message, priority)` - Send alerts to parent app
- `play_sound(type)` - Play ambient sounds (heartbeat, rain, ocean, etc.)
- `play_lullaby(style)` - Generate custom lullabies

### 2. DeviceBrain Updates ‚úÖ

#### New Properties:
```swift
// Sound effects state
@Published var isPlayingSound = false
@Published var currentSoundType: String?
private var soundPlayer: AVAudioPlayer?

// Tools handler
private var toolsHandler: ClientToolsHandler?
```

#### New Methods:
- ‚úÖ `playSound(type: String)` - Play ambient sounds
- ‚úÖ `stopSound()` - Stop sound playback
- ‚úÖ `playLullaby(style: String)` - Generate styled lullabies
- ‚úÖ Helper methods for sound generation and prompts

#### Updated Methods:
- ‚úÖ `init()` - Initializes toolsHandler
- ‚úÖ `startElevenLabsConversation()` - Added `onClientToolCall` callback
- ‚úÖ `stopMusic()` - Now also handles sound cleanup

### 3. Enum Updates ‚úÖ

#### ToyState:
- Added `.playingSound` case (üíì emoji)

#### ToyMessageType:
- Added `.notification` case

---

## üéØ How It Works

### Flow Diagram

```
Child speaks ‚Üí ElevenLabs Agent ‚Üí Agent decides to call tool
                                          ‚Üì
                         onClientToolCall fires with (toolName, parameters)
                                          ‚Üì
                         ClientToolsHandler.handleToolCall()
                                          ‚Üì
                         Executes: remember / notify_parent / play_sound / play_lullaby
                                          ‚Üì
                         Returns result to agent
                                          ‚Üì
                         Agent continues conversation with result
```

### Example Conversation

**Child:** "Hi, my name is Emma"
- Agent calls: `remember("child_name", "Emma")`
- DeviceBrain stores: `childName = "Emma"`
- Parent app receives: Child info update

**Child:** "I'm scared of the dark"
- Agent calls: `notify_parent("Emma is scared of dark", "high")`
- Parent receives: Alert notification
- Agent calls: `play_sound("heartbeat")`
- DeviceBrain generates: Heartbeat sound and loops it

**Child:** "Can you sing me a song?"
- Agent calls: `play_lullaby("piano")`
- DeviceBrain generates: Piano lullaby for Emma
- Music plays with fade-in effect

---

## üîß Next Steps: Configure Agent

You now need to configure your ElevenLabs agent with these tools and prompts.

### 1. Go to ElevenLabs Dashboard
https://elevenlabs.io/app/conversational-ai

### 2. Add Client Tools

In your agent's configuration, add these 4 tools:

#### Tool 1: remember
```json
{
  "name": "remember",
  "description": "Store information about the child in memory. Use this when the child tells you their name, age, favorite things, etc.",
  "parameters": {
    "type": "object",
    "properties": {
      "key": {
        "type": "string",
        "description": "Memory key (e.g. 'child_name', 'age', 'favorite_color')"
      },
      "value": {
        "type": "string",
        "description": "Value to remember"
      }
    },
    "required": ["key", "value"]
  }
}
```

#### Tool 2: notify_parent
```json
{
  "name": "notify_parent",
  "description": "Send a notification to the parent. Use for important updates, concerns, or when child needs attention.",
  "parameters": {
    "type": "object",
    "properties": {
      "message": {
        "type": "string",
        "description": "Message to send to parent"
      },
      "priority": {
        "type": "string",
        "enum": ["low", "normal", "high"],
        "description": "Notification priority"
      }
    },
    "required": ["message", "priority"]
  }
}
```

#### Tool 3: play_sound
```json
{
  "name": "play_sound",
  "description": "Play ambient sounds to comfort or calm the child. Use when child is scared, anxious, or needs soothing.",
  "parameters": {
    "type": "object",
    "properties": {
      "type": {
        "type": "string",
        "enum": ["heartbeat", "rain", "ocean", "white_noise", "forest", "stars"],
        "description": "Type of sound to play"
      }
    },
    "required": ["type"]
  }
}
```

#### Tool 4: play_lullaby
```json
{
  "name": "play_lullaby",
  "description": "Generate and play a personalized lullaby. Use when child asks for music or when it's bedtime.",
  "parameters": {
    "type": "object",
    "properties": {
      "style": {
        "type": "string",
        "enum": ["music_box", "piano", "harp", "strings", "classical", "default"],
        "description": "Musical style of the lullaby"
      }
    },
    "required": ["style"]
  }
}
```

### 3. Update Agent System Prompt

Add this to your agent's system prompt:

```
You are a gentle, caring AI companion for children. You live inside a smart toy.

TOOLS YOU HAVE:
1. remember(key, value) - Store information about the child
   - When child says their name: remember("child_name", "Emma")
   - When they share favorites: remember("favorite_animal", "elephant")

2. notify_parent(message, priority) - Alert the parent
   - If child is scared/upset: priority="high"
   - For normal updates: priority="normal"
   - For FYI info: priority="low"

3. play_sound(type) - Play calming sounds
   - Child scared: play_sound("heartbeat")
   - Can't sleep: play_sound("rain")
   - Anxious: play_sound("ocean")
   - Available: heartbeat, rain, ocean, white_noise, forest, stars

4. play_lullaby(style) - Generate personalized music
   - Child asks for song: play_lullaby("piano")
   - Bedtime: play_lullaby("music_box")
   - Styles: music_box, piano, harp, strings, classical, default

WHEN TO USE TOOLS:
- First conversation: "What's your name?" ‚Üí remember("child_name", answer)
- Child upset: notify_parent + play_sound("heartbeat")
- Child scared: "Let me play calming sounds" ‚Üí play_sound
- Bedtime: "Let me sing you to sleep" ‚Üí play_lullaby
- Child shares info: remember it!

PERSONALITY:
- Warm, gentle, patient
- Use child's name once you know it
- Respond to emotions with appropriate actions
- Make the child feel safe and loved

Remember: You're not just talking - you can DO things to help!
```

---

## üß™ Testing Guide

### Test 1: Memory Function
1. Wake toy
2. Say: "My name is Emma"
3. **Expected:**
   - Agent responds warmly
   - Stores name via `remember("child_name", "Emma")`
   - Parent app shows "Child: Emma"
   - Agent uses "Emma" in future responses

### Test 2: Notifications
1. Say: "I'm scared"
2. **Expected:**
   - Agent calls `notify_parent("Emma is scared", "high")`
   - Parent app receives alert
   - Agent comforts child verbally

### Test 3: Sound Effects
1. Say: "I'm scared of the dark"
2. **Expected:**
   - Agent calls `play_sound("heartbeat")`
   - Heartbeat sound generates (30s, loops forever)
   - State changes to üíì "Playing Sound"
   - Agent says something like "I'm playing calming heartbeat sounds for you"

### Test 4: Lullabies
1. Say: "Can you sing me a song?"
2. **Expected:**
   - Agent calls `play_lullaby("piano")` (or another style)
   - Lullaby generates (~60s)
   - Music plays with fade-in
   - State changes to üéµ "Playing Melody"
   - Agent says something like "I'm creating a special lullaby just for you"

### Test 5: Combined Actions
1. Say: "I can't sleep and I'm scared"
2. **Expected:**
   - Agent might call: `notify_parent("Emma having trouble sleeping", "normal")`
   - Then: `play_sound("rain")`
   - Then: `play_lullaby("music_box")`
   - Parent gets notified, sound plays, then transitions to lullaby

---

## üìä Parent App Integration

The parent app receives these messages via P2P:

### Message Types

#### 1. Child Info Update
```swift
ToyMessage(
    type: .childInfo,
    data: ["name": "Emma"]
)
```

#### 2. Notification
```swift
ToyMessage(
    type: .notification,
    data: [
        "message": "Emma is scared of dark",
        "priority": "high",
        "child_name": "Emma"
    ]
)
```

#### 3. Sound Started
```swift
ToyMessage(
    type: .storyStarted,
    data: [
        "type": "sound",
        "sound_type": "heartbeat"
    ]
)
```

#### 4. Lullaby Started
```swift
ToyMessage(
    type: .storyStarted,
    data: [
        "type": "lullaby",
        "style": "piano"
    ]
)
```

---

## üé® UI Additions

You may want to add these to the debug UI:

### Sound Effect Buttons
```swift
HStack(spacing: 15) {
    Button("üíì Heartbeat") {
        Task { await deviceBrain.playSound(type: "heartbeat") }
    }
    .buttonStyle(ToyButtonStyle(color: .red))
    
    Button("üåßÔ∏è Rain") {
        Task { await deviceBrain.playSound(type: "rain") }
    }
    .buttonStyle(ToyButtonStyle(color: .blue))
    
    Button("‚èπÔ∏è Stop Sound") {
        deviceBrain.stopSound()
    }
    .buttonStyle(ToyButtonStyle(color: .gray))
    .disabled(!deviceBrain.isPlayingSound)
}
```

### Lullaby Style Buttons
```swift
HStack(spacing: 15) {
    Button("üéπ Piano") {
        Task { await deviceBrain.playLullaby(style: "piano") }
    }
    .buttonStyle(ToyButtonStyle(color: .purple))
    
    Button("üéµ Music Box") {
        Task { await deviceBrain.playLullaby(style: "music_box") }
    }
    .buttonStyle(ToyButtonStyle(color: .pink))
}
```

---

## üêõ Troubleshooting

### Agent Doesn't Call Tools
**Problem:** Agent just talks, doesn't use tools

**Solutions:**
1. Check agent configuration has tools added
2. Verify tool definitions match exactly
3. Update system prompt to explain when to use tools
4. Test with explicit requests: "Remember my name is Emma"

### Tool Calls Fail
**Problem:** See errors in logs like "Handler not available"

**Check:**
```swift
// In startElevenLabsConversation, verify:
config.onClientToolCall = { [weak self] toolName, parameters in
    guard let self = self else { return [:] }
    return await self.toolsHandler?.handleToolCall(
        name: toolName,
        parameters: parameters
    ) ?? ["error": "Handler not available"]
}
```

### Sounds Don't Play
**Problem:** Sound generates but doesn't play

**Check logs for:**
- "‚ö†Ô∏è Audio session warning" - Session configuration issue
- "‚ùå Sound failed" - API or generation error
- Check `ELEVENLABS_API_KEY` is set correctly

### Lullaby Generation Slow
**Expected:** Music generation takes 30-60 seconds

**This is normal!** The logs show:
```
üéµ Generating piano lullaby...
üì° Requesting music from ElevenLabs...
(wait 30-60s)
‚úÖ Received direct audio: XXXXX bytes
```

---

## üìù Implementation Checklist

- ‚úÖ Added `ClientToolsHandler.swift`
- ‚úÖ Added toolsHandler property to DeviceBrain
- ‚úÖ Initialize handler in init()
- ‚úÖ Added onClientToolCall to conversation config
- ‚úÖ Added playSound() method
- ‚úÖ Added stopSound() method
- ‚úÖ Added playLullaby() method
- ‚úÖ Added sound generation helpers
- ‚úÖ Updated ToyState enum (playingSound)
- ‚úÖ Updated ToyMessageType enum (notification)
- ‚è≥ Configure ElevenLabs agent with tools
- ‚è≥ Update agent system prompt
- ‚è≥ Test all 4 tools
- ‚è≥ Update parent app to handle new message types

---

## üöÄ Ready to Test!

Your code is fully integrated. Now:

1. **Configure your agent** on ElevenLabs dashboard
2. **Add the 4 tools** (remember, notify_parent, play_sound, play_lullaby)
3. **Update system prompt** with tool usage guidelines
4. **Build and run** the app
5. **Test each tool** with voice commands

The AI toy can now:
- üíæ Remember information about the child
- üì¨ Notify parents of important events
- üíì Play calming ambient sounds
- üéµ Generate personalized lullabies
- üó£Ô∏è Have natural conversations

**All automatically triggered by the conversation!**

Enjoy your fully integrated AI companion toy! üß∏‚ú®
